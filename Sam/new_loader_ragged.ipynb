{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "979e9bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\huang\\miniconda3\\envs\\gpu-ml-base\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "c:\\Users\\huang\\miniconda3\\envs\\gpu-ml-base\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "class TCRDataLoader:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.aa_dict = None\n",
    "        self.v_dict = None\n",
    "        self.j_dict = None\n",
    "        self.label_map = None\n",
    "        self.max_length = None\n",
    "        \n",
    "    def load_files(self):\n",
    "        df_list = []\n",
    "        files = glob(self.data_dir + \"/*/*.tsv\")\n",
    "        \n",
    "        for file_path in files:\n",
    "            df = pd.read_csv(file_path, sep='\\t')\n",
    "            \n",
    "            # Extract folder name from file path\n",
    "            folder_name = os.path.basename(os.path.dirname(file_path))\n",
    "            \n",
    "            # Split folder name to get Antigen\n",
    "            # \"HLA-A-CMV\" â†’ [\"HLA\", \"A\", \"CMV\"]\n",
    "            parts = folder_name.split('-')\n",
    "            antigen = parts[-1]  # Get last part (the antigen)\n",
    "            \n",
    "            # Add antigen column to dataframe\n",
    "            df['Antigen'] = antigen\n",
    "            \n",
    "            df_list.append(df)\n",
    "        \n",
    "        combined_df = pd.concat(df_list, ignore_index=True)\n",
    "        return combined_df\n",
    "    \n",
    "    def build_vocabulary(self, df):\n",
    "        # Amino acids\n",
    "        all_sequences = df['aminoAcid'].dropna()\n",
    "        giant_string = ''.join(all_sequences)\n",
    "        unique_letters = sorted(set(giant_string))\n",
    "        self.aa_dict = {}\n",
    "        for i, letter in enumerate(unique_letters):\n",
    "            self.aa_dict[letter] = i+1\n",
    "        \n",
    "        # V genes\n",
    "        v_genes = df['v_beta'].dropna().unique()\n",
    "        v_genes = sorted(v_genes)\n",
    "        self.v_dict = {}\n",
    "        for i, gene in enumerate(v_genes):\n",
    "            self.v_dict[gene] = i + 1\n",
    "\n",
    "        # J genes\n",
    "        j_genes = df['j_beta'].dropna().unique()\n",
    "        j_genes = sorted(j_genes)\n",
    "        self.j_dict = {}\n",
    "        for i, gene in enumerate(j_genes):\n",
    "            self.j_dict[gene] = i + 1\n",
    "\n",
    "        # Labels\n",
    "        antigens = df['Antigen'].unique()\n",
    "        self.label_map = {}\n",
    "        for i, antigen in enumerate(sorted(antigens)):\n",
    "            self.label_map[antigen] = i\n",
    "\n",
    "        # Max length (keep for reference, but won't use for padding)\n",
    "        sequence_lengths = [len(seq) for seq in df['aminoAcid'].dropna()]\n",
    "        self.max_length = max(sequence_lengths)\n",
    "    \n",
    "    def encode_sequences_ragged(self, sequences):\n",
    "        \"\"\"Encode sequences without padding for ragged tensors\"\"\"\n",
    "        encoded_aa = []\n",
    "        for sequence in sequences:\n",
    "            sequence_numbers = []\n",
    "            for letter in sequence:\n",
    "                number = self.aa_dict.get(letter, 0)  # 0 if letter not found\n",
    "                sequence_numbers.append(number)\n",
    "            # NO PADDING - keep original length\n",
    "            encoded_aa.append(sequence_numbers)\n",
    "        return encoded_aa\n",
    "    \n",
    "    def encode_v_genes(self, v_genes):\n",
    "        encoded_v = []\n",
    "        for v_gene in v_genes:\n",
    "            encoded_v.append(self.v_dict.get(v_gene, 0))\n",
    "        return encoded_v\n",
    "    \n",
    "    def encode_j_genes(self, j_genes):\n",
    "        encoded_j = []\n",
    "        for j_gene in j_genes:\n",
    "            encoded_j.append(self.j_dict.get(j_gene, 0))\n",
    "        return encoded_j\n",
    "    \n",
    "    def load_and_encode_data(self, batch_size=100, shuffle=True):\n",
    "        # 1. Load files\n",
    "        df = self.load_files()\n",
    "        \n",
    "        # 2. Build vocabularies\n",
    "        self.build_vocabulary(df)\n",
    "        \n",
    "        # 3. Filter out rows with missing amino acid sequences AND very short sequences\n",
    "        valid_rows = df['aminoAcid'].notna()\n",
    "        df_valid = df[valid_rows].copy()\n",
    "        \n",
    "        # Filter out very short sequences that would cause issues with CNN\n",
    "        min_length = 10\n",
    "        long_enough = df_valid['aminoAcid'].str.len() >= min_length\n",
    "        df_valid = df_valid[long_enough].copy()\n",
    "        \n",
    "        print(f\"Filtered dataset: {len(df_valid)} sequences (min length: {min_length})\")\n",
    "        \n",
    "        # 4. Get the data you want to encode\n",
    "        sequences = df_valid['aminoAcid'].values\n",
    "        v_genes = df_valid['v_beta'].fillna('UNK').values\n",
    "        j_genes = df_valid['j_beta'].fillna('UNK').values\n",
    "        labels = [self.label_map[antigen] for antigen in df_valid['Antigen']]\n",
    "        \n",
    "        # 5. Encode everything (NO PADDING)\n",
    "        X_sequences = self.encode_sequences_ragged(sequences)\n",
    "        X_v_genes = self.encode_v_genes(v_genes)\n",
    "        X_j_genes = self.encode_j_genes(j_genes)\n",
    "        \n",
    "        # 6. Create ragged tensors\n",
    "        X_sequences_ragged = tf.ragged.constant(X_sequences, dtype=tf.int32)\n",
    "        X_v_genes_tensor = tf.constant(X_v_genes, dtype=tf.int32)\n",
    "        X_j_genes_tensor = tf.constant(X_j_genes, dtype=tf.int32)\n",
    "        y_labels_tensor = tf.constant(labels, dtype=tf.int32)\n",
    "        \n",
    "        # 7. Create TensorFlow dataset from ragged tensors\n",
    "        dataset = tf.data.Dataset.from_tensor_slices({\n",
    "            'cdr3_sequence': X_sequences_ragged,\n",
    "            'v_gene': X_v_genes_tensor,\n",
    "            'j_gene': X_j_genes_tensor,\n",
    "            'labels': y_labels_tensor\n",
    "        })\n",
    "        \n",
    "        # Map to the format expected by the model: ((inputs), labels)\n",
    "        dataset = dataset.map(lambda x: (\n",
    "            (x['cdr3_sequence'], x['v_gene'], x['j_gene']),\n",
    "            tf.one_hot(x['labels'], len(self.label_map))\n",
    "        ))\n",
    "        \n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(len(X_sequences))\n",
    "        \n",
    "        # Batch ragged tensors\n",
    "        dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "    def get_vocab_sizes(self):\n",
    "        \"\"\"Return vocabulary sizes for model construction\"\"\"\n",
    "        return {\n",
    "            'aa_vocab_size': len(self.aa_dict) + 1,  # +1 for padding/unknown\n",
    "            'v_vocab_size': len(self.v_dict) + 1,    # +1 for unknown\n",
    "            'j_vocab_size': len(self.j_dict) + 1,    # +1 for unknown\n",
    "            'num_classes': len(self.label_map),\n",
    "            'max_length': self.max_length\n",
    "        }\n",
    "    \n",
    "    def get_mappings(self):\n",
    "        \"\"\"Return the created mappings for inspection\"\"\"\n",
    "        return {\n",
    "            'aa_dict': self.aa_dict,\n",
    "            'v_dict': self.v_dict,\n",
    "            'j_dict': self.j_dict,\n",
    "            'label_map': self.label_map\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2090fce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tcr_model_cnn(vocab_sizes):\n",
    "    \"\"\"Create a CNN model that handles ragged tensors by converting to dense\"\"\"\n",
    "    # Input layers for ragged tensors\n",
    "    cdr3_input = tf.keras.Input(shape=[None], dtype=tf.int32, name='cdr3_sequence')\n",
    "    v_gene_input = tf.keras.Input(shape=(), dtype=tf.int32, name='v_gene')\n",
    "    j_gene_input = tf.keras.Input(shape=(), dtype=tf.int32, name='j_gene')\n",
    "    \n",
    "    # Convert ragged tensor to dense tensor for CNN processing\n",
    "    # This adds minimal padding per batch\n",
    "    cdr3_dense = cdr3_input.to_tensor(default_value=0)\n",
    "    \n",
    "    # CDR3 sequence encoder\n",
    "    cdr3_embed = tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_sizes['aa_vocab_size'], \n",
    "        output_dim=64, \n",
    "        mask_zero=True  # Handle the padding we just added\n",
    "    )(cdr3_dense)\n",
    "    \n",
    "    # CNN layers with smaller kernels to handle shorter sequences\n",
    "    conv1 = tf.keras.layers.Conv1D(\n",
    "        filters=64, \n",
    "        kernel_size=3,\n",
    "        strides=1, \n",
    "        padding='valid',\n",
    "        activation='relu'\n",
    "    )(cdr3_embed)\n",
    "    \n",
    "    conv2 = tf.keras.layers.Conv1D(\n",
    "        filters=128, \n",
    "        kernel_size=3,\n",
    "        strides=1, \n",
    "        padding='valid',\n",
    "        activation='relu'\n",
    "    )(conv1)\n",
    "    \n",
    "    conv3 = tf.keras.layers.Conv1D(\n",
    "        filters=256, \n",
    "        kernel_size=2,\n",
    "        strides=1, \n",
    "        padding='valid',\n",
    "        activation='relu'\n",
    "    )(conv2)\n",
    "    \n",
    "    # Global pooling to handle remaining variable lengths\n",
    "    cdr3_encoded = tf.keras.layers.GlobalMaxPooling1D()(conv3)\n",
    "    \n",
    "    # Gene embeddings\n",
    "    v_embed = tf.keras.layers.Embedding(vocab_sizes['v_vocab_size'], 32)(v_gene_input)\n",
    "    j_embed = tf.keras.layers.Embedding(vocab_sizes['j_vocab_size'], 32)(j_gene_input)\n",
    "    \n",
    "    # Flatten gene embeddings\n",
    "    v_flat = tf.keras.layers.Flatten()(v_embed)\n",
    "    j_flat = tf.keras.layers.Flatten()(j_embed)\n",
    "    \n",
    "    # Concatenate all features\n",
    "    fused = tf.keras.layers.Concatenate()([cdr3_encoded, v_flat, j_flat])\n",
    "    \n",
    "    # Classifier\n",
    "    dropout1 = tf.keras.layers.Dropout(rate=0.1)(fused)\n",
    "    dense1 = tf.keras.layers.Dense(units=128, activation='relu')(dropout1)\n",
    "    dropout2 = tf.keras.layers.Dropout(rate=0.05)(dense1)\n",
    "    dense2 = tf.keras.layers.Dense(units=64, activation='relu')(dropout2)\n",
    "    output = tf.keras.layers.Dense(units=vocab_sizes['num_classes'], activation='softmax')(dense2)\n",
    "    \n",
    "    model = tf.keras.Model(\n",
    "        inputs=[cdr3_input, v_gene_input, j_gene_input],\n",
    "        outputs=output\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0992da11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Filtered dataset: 2063 sequences (min length: 10)\n",
      "Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize loader\n",
    "data_dir = 'Data/Human_Antigens'\n",
    "loader = TCRDataLoader(data_dir)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "dataset = loader.load_and_encode_data(batch_size=100, shuffle=True)\n",
    "print(\"Data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ec0d157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'KerasTensor' object has no attribute 'to_tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m vocab_sizes = loader.get_vocab_sizes()\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreating model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model = \u001b[43mcreate_tcr_model_cnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Compile model\u001b[39;00m\n\u001b[32m      7\u001b[39m model.compile(\n\u001b[32m      8\u001b[39m     optimizer=\u001b[33m'\u001b[39m\u001b[33madam\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m      9\u001b[39m     loss=\u001b[33m'\u001b[39m\u001b[33mcategorical_crossentropy\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     10\u001b[39m     metrics=[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     11\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mcreate_tcr_model_cnn\u001b[39m\u001b[34m(vocab_sizes)\u001b[39m\n\u001b[32m      6\u001b[39m j_gene_input = tf.keras.Input(shape=(), dtype=tf.int32, name=\u001b[33m'\u001b[39m\u001b[33mj_gene\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Convert ragged tensor to dense tensor for CNN processing\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# This adds minimal padding per batch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m cdr3_dense = \u001b[43mcdr3_input\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m(default_value=\u001b[32m0\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# CDR3 sequence encoder\u001b[39;00m\n\u001b[32m     13\u001b[39m cdr3_embed = tf.keras.layers.Embedding(\n\u001b[32m     14\u001b[39m     input_dim=vocab_sizes[\u001b[33m'\u001b[39m\u001b[33maa_vocab_size\u001b[39m\u001b[33m'\u001b[39m], \n\u001b[32m     15\u001b[39m     output_dim=\u001b[32m64\u001b[39m, \n\u001b[32m     16\u001b[39m     mask_zero=\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Handle the padding we just added\u001b[39;00m\n\u001b[32m     17\u001b[39m )(cdr3_dense)\n",
      "\u001b[31mAttributeError\u001b[39m: 'KerasTensor' object has no attribute 'to_tensor'"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "vocab_sizes = loader.get_vocab_sizes()\n",
    "print(\"Creating model...\")\n",
    "model = create_tcr_model_cnn(vocab_sizes)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1e3919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-ml-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
